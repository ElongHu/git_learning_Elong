import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn.preprocessing import StandardScaler
import optuna

# 读取CSV文件
file_path = 'data/20240920.csv'  # 替换为你的CSV文件路径
data = pd.read_csv(file_path)

# 数据筛选（例如选择特定季度）
data_q2 = data[(data['quarter_no_'] == 2) & (data['year_no_'] == 2024)]
print('q2------------------')
print(len(data_q2))

# 查看标签分布
label_counts = data['iscp'].value_counts()
print(label_counts)

# 指定特征与标签
features = [
    "avg_duration",
    "avg_duration_btw_35_40",
    "duration_30_ratio",
    "duration_80_ratio",
    "duration_90_ratio",
    "duration_50_ratio",
    "avg_duration_btw_80_90",
    "avg_duration_btw_30_35",
    "duration_60_ratio",
    "duration_40_ratio",
    "avg_duration_btw_20_25",
    "duration_25_ratio",
    "avg_duration_btw_60_70",
    "avg_duration_btw_25_30",
    "duration_70_ratio",
    "duration_5_ratio",
    "duration_15_ratio",
    "avg_duration_btw_5_10",
    "duration_20_ratio",
    "duration_10_ratio",
    "duration_18_ratio",
    "avg_duration_btw_40_50",
    "avg_duration_btw_10_15",
    "avg_duration_btw_15_20",
    "avg_duration_btw_50_60",
    "ratio_end",
    "avg_duration_btw_70_80",
    "avg_end_cnt",
    "cnt_call",
    "avg_call_cnt",
    "original_charge_real",
    "all_cnt",
    "cust_age",
    "is_end_bigger_10",
    "is_end_bigger_20",
    "is_end_bigger_30",
    "is_end_bigger_40",
    "is_end_bigger_50",
    "is_end_bigger_60",
    "is_end_bigger_70",
    "is_end_bigger_80",
    "is_end_bigger_90",
    "latn_nbr"
]
label = 'iscp'

# 分离特征和标签
X = data[features]
y = data[label]

# 数据标准化
scaler = StandardScaler()
X = scaler.fit_transform(X)

# 采用 stratify 保证正负样本比例一致
all_count = len(data)
train_ratio = 0.95
train_count = int(all_count * train_ratio)
test_count = int(all_count * (1 - train_ratio))
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_count, train_size=train_count, stratify=y)

print("训练集标签分布：\n", y_train.value_counts())
print("测试集标签分布：\n", y_test.value_counts())

# 转换为 PyTorch 张量
X_train = torch.tensor(X_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
y_test = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

# 计算正负样本权重（负样本数/正样本数）
pos_weight = (y_train == 0).sum().float() / (y_train == 1).sum().float()
print("正样本权重:", pos_weight.item())


# 定义自定义损失函数
class BCEWithLogitsLossWithPosWeight(nn.Module):
    def __init__(self, pos_weight):
        super(BCEWithLogitsLossWithPosWeight, self).__init__()
        self.pos_weight = pos_weight

    def forward(self, logits, targets):
        loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')
        weights = torch.where(targets == 1, self.pos_weight, torch.ones_like(self.pos_weight))
        loss = loss * weights
        return loss.mean()


# 定义支持超参数调节的 MLP 模型
class MLPClassifierAuto(nn.Module):
    def __init__(self, input_dim, n_units0, n_units1, n_units2, n_units3, n_units4, dropout_rate):
        super(MLPClassifierAuto, self).__init__()
        self.hidden0 = nn.Linear(input_dim, n_units0)
        self.hidden1 = nn.Linear(n_units0, n_units1)
        self.hidden2 = nn.Linear(n_units1, n_units2)
        self.hidden3 = nn.Linear(n_units2, n_units3)
        self.hidden4 = nn.Linear(n_units3, n_units4)
        self.output = nn.Linear(n_units4, 1)

        self.leaky_relu = nn.LeakyReLU(0.01)
        self.elu = nn.ELU()
        self.dropout = nn.Dropout(dropout_rate)

        self.batch_norm0 = nn.BatchNorm1d(n_units0)
        self.batch_norm1 = nn.BatchNorm1d(n_units1)
        self.batch_norm2 = nn.BatchNorm1d(n_units2)
        self.batch_norm3 = nn.BatchNorm1d(n_units3)
        self.batch_norm4 = nn.BatchNorm1d(n_units4)

    def forward(self, x):
        x = self.dropout(self.batch_norm0(self.leaky_relu(self.hidden0(x))))
        x = self.dropout(self.batch_norm1(self.elu(self.hidden1(x))))
        x = self.dropout(self.batch_norm2(self.leaky_relu(self.hidden2(x))))
        x = self.dropout(self.batch_norm3(self.elu(self.hidden3(x))))
        x = self.dropout(self.batch_norm4(self.leaky_relu(self.hidden4(x))))
        x = self.output(x)
        return x


input_dim = X_train.shape[1]


# 定义 Optuna 的目标函数
def objective(trial):
    # 采样各隐藏层神经元数量、dropout 率、学习率、权重衰减系数等超参数
    n_units0 = trial.suggest_int("n_units0", 128, 512, step=64)
    n_units1 = trial.suggest_int("n_units1", 128, 512, step=64)
    n_units2 = trial.suggest_int("n_units2", 64, 256, step=32)
    n_units3 = trial.suggest_int("n_units3", 64, 256, step=32)
    n_units4 = trial.suggest_int("n_units4", 16, 64, step=16)
    dropout_rate = trial.suggest_float("dropout_rate", 0.0, 0.5)
    lr = trial.suggest_loguniform("lr", 1e-5, 1e-2)
    weight_decay = trial.suggest_loguniform("weight_decay", 1e-6, 1e-2)

    # 构造模型
    model = MLPClassifierAuto(input_dim, n_units0, n_units1, n_units2, n_units3, n_units4, dropout_rate)
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    criterion = BCEWithLogitsLossWithPosWeight(pos_weight)

    epochs = 50  # 调参阶段采用较少的 epoch
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()

        # 报告中间结果以便 Optuna 裁剪
        trial.report(loss.item(), epoch)
        if trial.should_prune():
            raise optuna.exceptions.TrialPruned()

    # 在验证集上评估
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_test)
        val_loss = criterion(val_outputs, y_test)
    return val_loss.item()


# 创建并运行 Optuna study（调参试验数可根据需求调整）
study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=50)

print("最佳试验结果：")
trial = study.best_trial
print("  最低验证损失: ", trial.value)
print("  最优超参数: ")
for key, value in trial.params.items():
    print(f"    {key}: {value}")

# 使用最优超参数训练最终模型
best_params = trial.params
model = MLPClassifierAuto(
    input_dim,
    best_params["n_units0"],
    best_params["n_units1"],
    best_params["n_units2"],
    best_params["n_units3"],
    best_params["n_units4"],
    best_params["dropout_rate"]
)
optimizer = optim.Adam(model.parameters(), lr=best_params["lr"], weight_decay=best_params["weight_decay"])
criterion = BCEWithLogitsLossWithPosWeight(pos_weight)

final_epochs = 100  # 最终训练更多 epoch
for epoch in range(final_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    if (epoch + 1) % 5 == 0:
        print(f'最终模型 Epoch [{epoch + 1}/{final_epochs}], Loss: {loss.item():.4f}')

# 测试最终模型效果
model.eval()
with torch.no_grad():
    predictions = torch.sigmoid(model(X_test))
    predictions = (predictions >= 0.5).float()
    accuracy = (predictions.eq(y_test).sum()) / float(y_test.shape[0])
    print(f'最终模型 Accuracy: {accuracy:.4f}')

# 保存最终模型
torch.save(model.state_dict(), 'model/SOTA_007_automl.pth')
